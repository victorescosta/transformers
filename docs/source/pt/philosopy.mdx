<!--Copyright 2020 A equipe HuggingFace. Todos os direitos reservados.

Licenciado sob a Licen√ßa Apache, Vers√£o 2.0 (a "Licen√ßa"); voc√™ n√£o pode usar este arquivo exceto em conformidade com
a licen√ßa. Voc√™ pode obter uma c√≥pia da Licen√ßa em

http://www.apache.org/licenses/LICENSE-2.0

A menos que exigido pela lei aplic√°vel ou acordado por escrito, o software distribu√≠do sob a Licen√ßa √© distribu√≠do em
uma BASE "AS IS", SEM GARANTIAS OU CONDI√á√ïES DE QUALQUER TIPO, expressas ou impl√≠citas. Consulte a Licen√ßa para o
idioma espec√≠fico que rege as permiss√µes e limita√ß√µes sob a Licen√ßa.
-->

# Filosofia

ü§ó Transformers √© uma biblioteca obstinada e constru√≠da para:

- Pesquisadores em NLP e educadores que buscam usar/estudar/estender modelos transformers em larga escala
- Praticantes que querem fazer ajustes finos nesses modelos e/ou usa-los em produ√ß√£o
- Engenheiros que apenas querem baixar um modelo pr√©-treinado e resolver algumas tarefas dadas de NLP.

A biblioteca foi constru√≠da com dois grandes objetivos em mente:

- Ser mais f√°cil e r√°pida de usar poss√≠vel:

  - Limitamos fortemente o n√∫mero de abstra√ß√µes voltadas para o usu√°rio para aprender, de fato, quase n√£o h√° abstra√ß√µes, apenas tr√™s classes padr√£o necess√°rias para usar cada modelo: [configuration](main_classes/configuration),
    [models](main_classes/model) e [tokenizer](main_classes/tokenizer).
  - Todas essas classes podem ser inicializadas de forma simples e unificada a partir de inst√¢ncias pr√©-treinadas usando um m√©todo comum de instancia√ß√£o `from_pretrained()` que cuidar√° do download (se necess√°rio), cache e carregamento da inst√¢ncia de classe relacionada e dados associados (configura√ß√µes de hiperpar√¢metros, vocabul√°rio de tokenizers e pesos de modelos) de um checkpoint pr√©-treinado fornecido no [Hugging Face Hub](https://huggingface.co/models) ou em seu pr√≥prio checkpoint salvo.
  - Al√©m dessas tr√™s classes base, a biblioteca fornece duas APIs: [`pipeline`] para usar rapidamente um modelo (mais seu tokenizer e configura√ß√£o associados) em uma determinada tarefa e [`Trainer`]/`Keras.fit` para rapidamente treinar ou ajustar um determinado modelo.
  - Como consequ√™ncia, esta biblioteca N√ÉO √© uma caixa de ferramentas modular de blocos de constru√ß√£o para redes neurais. Se voc√™ quiser estender/construir a biblioteca, basta usar m√≥dulos Python/PyTorch/TensorFlow/Keras regulares e herdar das classes base da biblioteca para reutilizar funcionalidades como carregamento/salvamento de modelo.

- Fornecer modelos de √∫ltima gera√ß√£o com desempenhos o mais pr√≥ximo poss√≠vel dos modelos originais:

  - Fornecemos pelo menos um exemplo para cada arquitetura que reproduz um resultado fornecido pelos autores oficiais da referida arquitetura.
  - O c√≥digo geralmente √© o mais pr√≥ximo poss√≠vel da base de c√≥digo original, o que significa que alguns c√≥digos PyTorch podem n√£o ser t√£o *pytorchico* quanto poderiam ser como resultado da convers√£o do c√≥digo TensorFlow e vice-versa.

Alguns outros objetivos:

- Expor as partes internas dos modelos da forma mais consistente poss√≠vel:

  - Damos acesso, usando uma √∫nica API, a todos os estados ocultos e pesos de aten√ß√£o.
  - A API do tokenizer e do modelo base √© padronizada para alternar facilmente entre os modelos.

- Incorporar uma sele√ß√£o subjetiva de ferramentas promissoras para refinar/investigar esses modelos:

  - Uma maneira simples/consistente de adicionar novos tokens ao vocabul√°rio e embeddings para ajuste fino.
  - Maneiras simples de mascarar e podar cabe√ßas de transformers.

- Alterne facilmente entre PyTorch e TensorFlow 2.0, permitindo treinamento usando uma estrutura de uma e infer√™ncia de outra.

## Principais conceitos

A biblioteca √© constru√≠da em torno de tr√™s tipos de classes para cada modelo:

- **Classes de Modelos** como a [`BertModel`], que s√£o mais de 30 modelos PyTorch ([torch.nn.Module](https://pytorch.org/docs/stable nn.html#torch.nn.Module)) ou modelos Keras ([ tf.keras.Model] https://www.tensorflow.org/api_docs/python/tf/keras/Model)) que funcionam com os pesos pr√©-treinados fornecidos na biblioteca.
- **Classes de Configura√ß√£o** como a [`BertConfig`], que armazenam todos os par√¢metros necess√°rios para construir um modelo. Voc√™ nem sempre precisa instanciar isso sozinho. Em particular, se voc√™ estiver usando um modelo pr√©-treinado sem nenhuma modifica√ß√£o, a cria√ß√£o do modelo cuidar√° automaticamente da instancia√ß√£o da configura√ß√£o (que faz parte do modelo).
- **Classes de Tokenizers** como a [`BertTokenizer`], que armazenam o vocabul√°rio para cada modelo e fornecem m√©todos para codificar/decodificar strings em uma lista de √≠ndices de incorpora√ß√£o de tokens a serem alimentados em um modelo.

Todas essas classes podem ser instanciadas a partir de inst√¢ncias pr√©-treinadas e salvas localmente usando dois m√©todos:

- `from_pretrained()` permite instanciar um modelo/configura√ß√£o/tokenizer de uma vers√£o pr√©-treinada fornecida pela pr√≥pria biblioteca (os modelos suportados podem ser encontrados no [Model Hub](https://huggingface.co/models)) ou armazenados localmente (ou em um servidor) pelo usu√°rio,
- `save_pretrained()` permite que voc√™ salve um modelo/configura√ß√£o/tokenizer localmente para que possa ser recarregado usando
  `from_pretrained()`.

